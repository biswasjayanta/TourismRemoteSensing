{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/USER/OneDrive - The University of Memphis/Khulna University/Paper-3/Processed/Excel/Confusion2022.csv')\n",
    "true_labels = df['GrndTrth']\n",
    "predicted_labels = df['Classified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[150   0   0   0   0   0]\n",
      " [  0 147   0   0   3   0]\n",
      " [  0   0 150   0   0   0]\n",
      " [  0   0   1 148   1   0]\n",
      " [  0   3   0   0 147   0]\n",
      " [  0   0   1   0   1 148]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producer Accuracy (Recall): [1.         0.98       1.         0.98666667 0.98       0.98666667]\n",
      "User Accuracy (Precision): [1.         0.98       0.98684211 1.         0.96710526 1.        ]\n"
     ]
    }
   ],
   "source": [
    "producer_accuracy = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "user_accuracy = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
    "\n",
    "print(\"\\nProducer Accuracy (Recall):\", producer_accuracy)\n",
    "print(\"User Accuracy (Precision):\", user_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"\\nOverall Accuracy:\", overall_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kappa Statistic: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
    "print(\"\\nKappa Statistic:\", kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Micro): 0.9888888888888889\n",
      "F1 Score (Macro): 0.9889107664044327\n",
      "F1 Score (Weighted): 0.9889107664044328\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 Score for each class (micro, macro, weighted)\n",
    "f1_micro = f1_score(true_labels, predicted_labels, average='micro')\n",
    "f1_macro = f1_score(true_labels, predicted_labels, average='macro')\n",
    "f1_weighted = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"F1 Score (Micro):\", f1_micro)\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
